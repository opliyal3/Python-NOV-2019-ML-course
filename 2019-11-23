import numpy
from keras.layers import Dense
from keras.models import Sequential
import os
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV

print(os.getcwd())

dataset1 = numpy.loadtxt('data\\diabetes.csv', delimiter=',', skiprows=1)
print(type(dataset1), dataset1.shape)

inputList = dataset1[:, 0:8]
resultList = dataset1[:, 8]
print(inputList[:5])
print(numpy.unique(resultList))


def create_default_model(optimizer='adam', init='uniform'): #1
    model = Sequential()
    model.add(Dense(14, input_dim=8, kernel_initializer=init, activation='relu')) # 2
    model.add(Dense(12, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) # 3
    print(model.summary())
    return model


# , epochs=200, batch_size=20, ==> remove
model = KerasClassifier(build_fn=create_default_model, verbose=0) # 4
# 5
optimizers = ['rmsprop', 'adam']
inits = ['normal', 'uniform']
epochs = [50, 100, 150]
batches = [5, 10, 15]
fiveFold = StratifiedKFold(n_splits=5, shuffle=True)

result = cross_val_score(model, inputList, resultList, cv=fiveFold)
print(f"result mean={result.mean()}, result std={result.std()}")
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import numpy
from keras.layers import Dense
from keras.models import Sequential
import os
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV

print(os.getcwd())

dataset1 = numpy.loadtxt('data\\diabetes.csv', delimiter=',', skiprows=1)
print(type(dataset1), dataset1.shape)

inputList = dataset1[:, 0:8]
resultList = dataset1[:, 8]
print(inputList[:5])
print(numpy.unique(resultList))


def create_default_model(optimizer='adam', init='uniform'): #1
    model = Sequential()
    model.add(Dense(14, input_dim=8, kernel_initializer=init, activation='relu')) # 2
    model.add(Dense(12, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) # 3
    print(model.summary())
    return model


# , epochs=200, batch_size=20, ==> remove
model = KerasClassifier(build_fn=create_default_model, verbose=0) # 4
# 5
optimizers = ['rmsprop', 'adam']
inits = ['normal', 'uniform']
epochs = [50, 100, 150]
batches = [5, 10, 15]
para_grid= dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init = inits)
grid = GridSearchCV(estimator=model, param_grid=para_grid)
grid_result = grid.fit(inputList, resultList)

cmd
cd notebook


jupyter-notebook --generate-config

cd ..\.jupyter
c = get_config()
c.NotebookApp.notebook_dir="c:\\Users\\Admin\\notebook"

any directory
jupyter-notebook

[first cell]
import numpy
from keras.layers import Dense
from keras.models import Sequential
import os
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV

print(os.getcwd())

dataset1 = numpy.loadtxt('data\\diabetes.csv', delimiter=',', skiprows=1)
print(type(dataset1), dataset1.shape)

inputList = dataset1[:, 0:8]
resultList = dataset1[:, 8]
print(inputList[:5])
print(numpy.unique(resultList))


def create_default_model(optimizer='adam', init='uniform'): #1
    model = Sequential()
    model.add(Dense(14, input_dim=8, kernel_initializer=init, activation='relu')) # 2
    model.add(Dense(12, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) # 3
    print(model.summary())
    return model


# , epochs=200, batch_size=20, ==> remove
model = KerasClassifier(build_fn=create_default_model, verbose=0) # 4
# 5
optimizers = ['rmsprop', 'adam']
inits = ['normal', 'uniform']
epochs = [50, 100, 150]
batches = [5, 10, 15]
para_grid= dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init = inits)
grid = GridSearchCV(estimator=model, param_grid=para_grid)
grid_result = grid.fit(inputList, resultList)

[second cell]
 type(grid_result)


~~~~~~~~~~~~~~~~~~~~~~~~
import numpy as np
from keras.datasets import boston_housing

(train_data, train_target), (test_data, test_target) = boston_housing.load_data()
print(train_data.shape, test_data.shape)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
means = grid_result.cv_results_['mean_test_score']
means

stds = grid_result.cv_results_['std_test_score']
stds

params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(f"[{param}] with mean={mean}, std={stdev}")

f'Best result:{grid_result.best_params_}, score={grid_result.best_score_}'
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import numpy as np
from keras.datasets import boston_housing

(train_data, train_target), (test_data, test_target) = boston_housing.load_data()
print(train_data.shape, test_data.shape)

mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data /= std
print(train_data.shape, test_data.shape)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import numpy as np
from keras import models
from keras import layers
from keras.datasets import boston_housing

(train_data, train_target), (test_data, test_target) = boston_housing.load_data()
print(train_data.shape, test_data.shape)

mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data /= std
print(train_data.shape, test_data.shape)


def build_model():
    model = models.Sequential()
    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
    model.add(layers.Dense(32, activation='relu'))
    model.add(layers.Dense(1))
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    model.summary()
    return model


model = build_model()
model.fit(train_data, train_target, validation_split=0.1, epochs=100, batch_size=10, verbose=1)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import numpy as np
from keras import models
from keras import layers
from keras.datasets import boston_housing

(train_data, train_target), (test_data, test_target) = boston_housing.load_data()
print(train_data.shape, test_data.shape)

mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data /= std
print(train_data.shape, test_data.shape)


def build_model():
    model = models.Sequential()
    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))
    model.add(layers.Dense(32, activation='relu'))
    model.add(layers.Dense(1))
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    model.summary()
    return model


model = build_model()
model.fit(train_data, train_target, validation_split=0.1, epochs=100, batch_size=10, verbose=1)

# for item in test_target:
#     print(item)

for (i,j) in zip(test_data, test_target):
    predict = model.predict(i.reshape(1, -1))
    print(f'actual={j}, predict as={predict}')
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from pandas import read_csv

dataFrame = read_csv('data\\iris.data', header=None)
print(type(dataFrame), dataFrame.shape)
print(dataFrame.columns)
print(dataFrame.index)
print(type(dataFrame.values))
print(dataFrame.values)
print(dataFrame.values[0,], type(dataFrame.values[0,][0]),type(dataFrame.values[0,][4]))

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from pandas import read_csv
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

dataFrame = read_csv('data\\iris.data', header=None)
print(type(dataFrame), dataFrame.shape)
print(dataFrame.columns)
print(dataFrame.index)
print(type(dataFrame.values))
print(dataFrame.values)
print(dataFrame.values[0,], type(dataFrame.values[0,][0]), type(dataFrame.values[0,][4]))

# cut data
dataset = dataFrame.values
features = dataset[:, 0:4].astype(float)
labels = dataset[:, 4]
print(features.mean(axis=0))

encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)
print(type(encoded_Y), encoded_Y[:10], encoded_Y[50:60], encoded_Y[100:110])
dummy_y =np_utils.to_categorical(encoded_Y)
print(type(dummy_y), dummy_y[:10], dummy_y[50:60], dummy_y[100:110])

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from keras import Sequential
from keras.layers import Dense
from sklearn import preprocessing
from pandas import read_csv
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

dataFrame = read_csv('data\\iris.data', header=None)
print(type(dataFrame), dataFrame.shape)
print(dataFrame.columns)
print(dataFrame.index)
print(type(dataFrame.values))
print(dataFrame.values)
print(dataFrame.values[0,], type(dataFrame.values[0,][0]), type(dataFrame.values[0,][4]))

# cut data
dataset = dataFrame.values
features = dataset[:, 0:4].astype(float)
labels = dataset[:, 4]
print(features.mean(axis=0))
result = preprocessing.scale(features, axis=0, with_mean=True)
print(result.mean(axis=0))

encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)
print(type(encoded_Y), encoded_Y[:10], encoded_Y[50:60], encoded_Y[100:110])
dummy_y = np_utils.to_categorical(encoded_Y)
print(type(dummy_y), dummy_y[:10], dummy_y[50:60], dummy_y[100:110])

def baseline_model():
    model = Sequential()
    model.add(Dense(8, input_dim=4, activation='relu'))
    model.add(Dense(12, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    model.summary()
    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])
    return model

baseline_model()


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
scores = [3.0, 4.0, 3.0]
import numpy as np


def manualSoftmax(x):
    x = np.array(x)
    return np.exp(x) / np.sum(np.exp(x), axis=0)


print(manualSoftmax(scores))

import tensorflow as tf

result2 = tf.nn.softmax(scores)
print(result2)

scores2 = [1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0]
result3 = tf.nn.softmax(scores2)
print(result3)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from keras import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn import preprocessing
from pandas import read_csv
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

dataFrame = read_csv('data\\iris.data', header=None)
print(type(dataFrame), dataFrame.shape)
print(dataFrame.columns)
print(dataFrame.index)
print(type(dataFrame.values))
print(dataFrame.values)
print(dataFrame.values[0,], type(dataFrame.values[0,][0]), type(dataFrame.values[0,][4]))

# cut data
dataset = dataFrame.values
features = dataset[:, 0:4].astype(float)
labels = dataset[:, 4]
print(features.mean(axis=0))
result = preprocessing.scale(features, axis=0, with_mean=True)
print(result.mean(axis=0))

encoder = LabelEncoder()
encoder.fit(labels)
encoded_Y = encoder.transform(labels)
print(type(encoded_Y), encoded_Y[:10], encoded_Y[50:60], encoded_Y[100:110])
dummy_y = np_utils.to_categorical(encoded_Y)
print(type(dummy_y), dummy_y[:10], dummy_y[50:60], dummy_y[100:110])


def baseline_model():
    model = Sequential()
    model.add(Dense(8, input_dim=4, activation='relu'))
    model.add(Dense(12, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    model.summary()
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=10, verbose=1)
Kfold = KFold(n_splits=3, shuffle=True)
results = cross_val_score(estimator, features, dummy_y, cv=Kfold)
print("accuracy: %.4f, std: %.4f"%(results.mean(), results.std()))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import numpy
from keras.datasets import imdb
from matplotlib import pyplot

(X_train, y_train), (X_test, y_test) = imdb.load_data()
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
print(numpy.unique(y_train), numpy.unique(y_test))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import numpy
from keras.datasets import imdb
from matplotlib import pyplot

(X_train, y_train), (X_test, y_test) = imdb.load_data()
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
print(numpy.unique(y_train), numpy.unique(y_test))
X = numpy.concatenate((X_train, X_test), axis=0)
y = numpy.concatenate((y_train, y_test), axis=0)
print(X[0])
print(X.shape)
print(y.shape)
print(len(numpy.unique(numpy.hstack(X))))
result = [len(x) for x in X]
print(result[:50])

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import numpy
from keras.datasets import imdb
from matplotlib import pyplot

(X_train, y_train), (X_test, y_test) = imdb.load_data()
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
print(numpy.unique(y_train), numpy.unique(y_test))
X = numpy.concatenate((X_train, X_test), axis=0)
y = numpy.concatenate((y_train, y_test), axis=0)
print(X[0])
print(X.shape)
print(y.shape)
print(len(numpy.unique(numpy.hstack(X))))
result = [len(x) for x in X]
print(result[:50])
print(f'comments mean={numpy.mean(result)}, std={numpy.std(result)}')

pyplot.subplot(121)
pyplot.boxplot(result)
pyplot.subplot(122)
pyplot.hist(result)
pyplot.show()
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from keras import layers
from keras import models
from keras.datasets import imdb
import numpy as np

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
print(train_data[0])
print(max([max(sequence) for sequence in train_data]))

word_index = imdb.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
for k in range(5):
    decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[k]])
    print(decoded_review)
print(train_labels[:5])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from keras import layers
from keras import models
from keras.datasets import imdb
import numpy as np

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
print(train_data[0])
print(max([max(sequence) for sequence in train_data]))

word_index = imdb.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
for k in range(5):
    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(decoded_review)
print(train_labels[:5])


def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
print(x_train[0])
~~~~~~~~~~~~~~~~~~~~
from keras import layers
from keras import models
from keras.datasets import imdb
import numpy as np

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
print(train_data[0])
print(max([max(sequence) for sequence in train_data]))

word_index = imdb.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
for k in range(5):
    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(decoded_review)
print(train_labels[:5])


def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
print(x_train[0])

model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])
model.summary()

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=128)


~~~~
from keras import layers
from keras import models
from keras.datasets import imdb
import numpy as np

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
print(train_data[0])
print(max([max(sequence) for sequence in train_data]))

word_index = imdb.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
for k in range(5):
    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(decoded_review)
print(train_labels[:5])


def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
print(x_train[0])

model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])
model.summary()

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=128)

~~~~~~~~~~~~~~~~~
from keras import layers
from keras import models
from keras.datasets import imdb
import numpy as np

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
print(train_data[0])
print(max([max(sequence) for sequence in train_data]))

word_index = imdb.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
for k in range(5):
    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(decoded_review)
print(train_labels[:5])


def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
print(x_train[0])

model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])
model.summary()

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=128, validation_data = (x_val, y_val))

history_dict = history.history
loss_value = history_dict['loss']
val_loss_value = history_dict['val_loss']
acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
epochs = range(1, len(acc)+1)

#matplotlib inline
import matplotlib.pyplot as plt
plt.plot(epochs, loss_value,'bo--', label='training loss')


#matplotlib inline
import matplotlib.pyplot as plt
plt.plot(epochs, loss_value,'bo--', label='training loss')
plt.plot(epochs, val_loss_value,'r*--', label='validation loss')
plt.legend()
plt.title('Training V.S. Validation')
plt.xlabel('epochs')
plt.ylabel('loss')

plt.plot(epochs, acc, 'bs-', label='training accuracy')
plt.plot(epochs, val_acc, 'r^-', label='validation accuracy')
plt.legend()
plt.title('Training/Validation accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')


plt.plot(epochs, loss_value,'go--', label='training loss')
plt.plot(epochs, acc, 'bs-', label='training accuracy')

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from keras.datasets import reuters
import numpy

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
print(len(train_data), len(test_data))
word_index = reuters.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
print(numpy.unique(train_labels))
print(train_labels[:5])
for k in range(5):
    news = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(news)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from keras.datasets import reuters
import numpy as np

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
print(len(train_data), len(test_data))
word_index = reuters.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
print(np.unique(train_labels))
print(train_labels[:5])
for k in range(5):
    news = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(news)


def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results


x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)


def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1
    return results

one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)
print(one_hot_train_labels[:5])

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
from keras.datasets import reuters
import numpy as np

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
print(len(train_data), len(test_data))
word_index = reuters.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
print(np.unique(train_labels))
print(train_labels[:5])
for k in range(5):
    news = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(news)


def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results


x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)


def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1
    return results


one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)
print(one_hot_train_labels[:5])

from keras import models, layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
~~~~~~~~~~~~~~~~~~~~
from keras.datasets import reuters
import numpy as np

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
print(len(train_data), len(test_data))
word_index = reuters.get_word_index()
reverse_word_index = dict([(v, k) for k, v in word_index.items()])
print(np.unique(train_labels))
print(train_labels[:5])
for k in range(5):
    news = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[k]])
    print(news)


def vectorize_sequence(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results


x_train = vectorize_sequence(train_data)
x_test = vectorize_sequence(test_data)


def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1
    return results


one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)
print(one_hot_train_labels[:5])

from keras import models, layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()


x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
history = model.fit(partial_x_train, partial_y_train, epochs = 20, batch_size=128, validation_data=(x_val,y_val))


#matplotlib inline
import matplotlib.pyplot as plt
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss)+1)
plt.plot(epochs, loss, 'bo-', label='training loss')
plt.plot(epochs, val_loss, 'r*-', label='validation loss')
plt.legend()


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'bo--', label='training accuracy')
plt.plot(epochs, val_acc, 'r*--', label='validation accuracy')
plt.legend()

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
scores = [0.3, 0.4, 0.3]
import numpy as np


def manualSoftmax(x):
    x = np.array(x)
    return np.exp(x) / np.sum(np.exp(x), axis=0)


print(manualSoftmax(scores))

import tensorflow as tf

result2 = tf.nn.softmax(scores)
print(result2)

scores2 = [0.1, 0.1, 0.1, 0.3, 0.1, 0.1, 0.1, 0.1]
result3 = tf.nn.softmax(scores2)
print(result3)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


y = 1000000000


def calculate(x):
    for i in range(0, 1000000):
        x += 0.0000001
    x -= 0.1
    return x


print(calculate(y))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import matplotlib.pyplot as plt
from keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
print(f"train data shape={train_images.shape}, test data shape={test_images.shape}")
print(f"train label size={len(train_labels)}, test label size={len(test_labels)}")

def plotImage(index):
    plt.title("The image marked as %d"%train_labels[index])
    plt.imshow(train_images[index], cmap='binary')
    plt.show()

def plotTestImage(index):
    plt.title('the image marked as %d'%test_labels[index])
    plt.imshow(test_images[index], cmap='binary')
    plt.show()

plotImage(9)
plotTestImage(10)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import keras.utils as utils

origs = [4, 7, 9, 0]
NUM_CATEGORY = 46

for orig in origs:
    converted = utils.to_categorical(orig, NUM_CATEGORY)
    print(f'after conversion, {orig} will become {converted}')
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
import numpy as np
from keras import datasets, utils

(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
print(train_images.shape)
flattenDim = 28 * 28
TRAINING_SIZE = len(train_images)
TEST_SIZE = len(test_images)

trainImages = np.reshape(train_images, (TRAINING_SIZE, flattenDim))
testImages = np.reshape(test_images, (TEST_SIZE, flattenDim))
print(type(trainImages[0]))

trainImages = trainImages.astype(np.float32)
testImages = testImages.astype(np.float32)
print(type(trainImages))
trainImages /= 255
testImages /= 255

NUM_DIGITS = 10
trainLabels = utils.to_categorical(train_labels, NUM_DIGITS)
testLabels = utils.to_categorical(test_labels, NUM_DIGITS)
print(trainLabels[0])
print(trainImages[0])

~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
import numpy as np
from keras import datasets, utils, layers, Sequential

(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
print(train_images.shape)
flattenDim = 28 * 28
TRAINING_SIZE = len(train_images)
TEST_SIZE = len(test_images)

trainImages = np.reshape(train_images, (TRAINING_SIZE, flattenDim))
testImages = np.reshape(test_images, (TEST_SIZE, flattenDim))
print(type(trainImages[0]))

trainImages = trainImages.astype(np.float32)
testImages = testImages.astype(np.float32)
print(type(trainImages))
trainImages /= 255
testImages /= 255

NUM_DIGITS = 10
trainLabels = utils.to_categorical(train_labels, NUM_DIGITS)
testLabels = utils.to_categorical(test_labels, NUM_DIGITS)
print(trainLabels[0])
print(trainImages[0])

model = Sequential()
model.add(layers.Dense(units=256, activation=tf.nn.relu, input_shape=(flattenDim,)))
model.add(layers.Dense(units=128, activation=tf.nn.relu))
model.add(layers.Dense(units=10, activation=tf.nn.softmax))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

model.fit(trainImages, trainLabels, epochs=20)


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()
print(train_images.shape, test_images.shape)
print(len(train_labels), len(test_labels))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()
print(train_images.shape, test_images.shape)
print(len(train_labels), len(test_labels))
print(np.unique(train_labels))

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()
print(train_images.shape, test_images.shape)
print(len(train_labels), len(test_labels))
print(np.unique(train_labels))

# plt.figure()
# plt.imshow(train_images[0])
# plt.colorbar()
# plt.grid(False)
# plt.show()
classNames = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',
              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']
train_images = train_images / 255.0
test_images = test_images / 255.0

plt.figure(figsize=(12, 8))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(classNames[train_labels[i]])
plt.show()
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt
from keras import Sequential
from keras import layers
from keras.layers import Flatten

(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()
print(train_images.shape, test_images.shape)
print(len(train_labels), len(test_labels))
print(np.unique(train_labels))

# plt.figure()
# plt.imshow(train_images[0])
# plt.colorbar()
# plt.grid(False)
# plt.show()
classNames = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',
              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']
train_images = train_images / 255.0
test_images = test_images / 255.0

# plt.figure(figsize=(12, 8))
# for i in range(25):
#     plt.subplot(5, 5, i + 1)
#     plt.xticks([])
#     plt.yticks([])
#     plt.grid(False)
#     plt.imshow(train_images[i], cmap=plt.cm.binary)
#     plt.xlabel(classNames[train_labels[i]])
# plt.show()

model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(layers.Dense(256, activation=tf.nn.relu))
model.add(layers.Dense(128, activation=tf.nn.relu))
model.add(layers.Dense(10, activation=tf.nn.softmax))
model.summary()
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=20)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt
from keras import Sequential
from keras import layers
from keras.layers import Flatten

(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()
print(train_images.shape, test_images.shape)
print(len(train_labels), len(test_labels))
print(np.unique(train_labels))

# plt.figure()
# plt.imshow(train_images[0])
# plt.colorbar()
# plt.grid(False)
# plt.show()
classNames = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',
              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']
train_images = train_images / 255.0
test_images = test_images / 255.0

# plt.figure(figsize=(12, 8))
# for i in range(25):
#     plt.subplot(5, 5, i + 1)
#     plt.xticks([])
#     plt.yticks([])
#     plt.grid(False)
#     plt.imshow(train_images[i], cmap=plt.cm.binary)
#     plt.xlabel(classNames[train_labels[i]])
# plt.show()

model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(layers.Dense(256, activation=tf.nn.relu))
model.add(layers.Dense(128, activation=tf.nn.relu))
model.add(layers.Dense(10, activation=tf.nn.softmax))
model.summary()
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=20)




test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)
f'test accuracy={test_accuracy}, loss={test_loss}'

num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
def plot_iamge(i, predictions_array, true_label, image):
    predictions_array, true_label, image=predictions_array, true_label[i], image[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(image, cmap=plt.cm.binary)


[prev cell]
predictions = model.predict(test_images)
predictions[:20]


num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
def plot_image(i, predictions_array, true_label, image):
    predictions_array, true_label, image=predictions_array, true_label[i], image[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(image, cmap=plt.cm.binary)
plt.figure(figsize=(3*2*num_cols, 3*num_rows))
for i in range(num_images):
    plt.subplot(num_rows, 2*num_cols, 2*i+1)
    plot_image(i, predictions[i], test_labels, test_images)